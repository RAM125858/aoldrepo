{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNTuk7i39XxYz33cvd+1Nj4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RAM125858/aoldrepo/blob/main/exp-5-.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVGJe5qUuuBP",
        "outputId": "491e3f8c-382e-47fa-cf6f-4a4ce9c12cc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All genres in Brown corpus:\n",
            "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n",
            "\n",
            "Number of unique words in 'editorial': 9109\n",
            "Sample words in 'editorial': ['!', '$1', '$1,000', '$1,000,000', '$1,250,000', '$1,450,000,000', '$1,750,000', '$1.1', '$133', '$15.5', '$150', '$2,000,000', '$2,300,000', '$200', '$3', '$3,000', '$3.15', '$3.22', '$30,000', '$4']\n",
            "\n",
            "Selected words [3493:3499]: ['freed', 'freedom', 'freedoms', 'freeholders', 'freely', 'freeman', \"freeman's\"]\n",
            "\n",
            "Word counts in selected genres:\n",
            "\n",
            "Genre: fiction\n",
            "freed: 2\n",
            "freedom: 2\n",
            "freedoms: 0\n",
            "freeholders: 0\n",
            "freely: 1\n",
            "freeman: 0\n",
            "freeman's: 0\n",
            "\n",
            "Genre: lore\n",
            "freed: 1\n",
            "freedom: 25\n",
            "freedoms: 1\n",
            "freeholders: 0\n",
            "freely: 3\n",
            "freeman: 0\n",
            "freeman's: 0\n",
            "\n",
            "Genre: belles_lettres\n",
            "freed: 2\n",
            "freedom: 29\n",
            "freedoms: 0\n",
            "freeholders: 1\n",
            "freely: 6\n",
            "freeman: 3\n",
            "freeman's: 0\n",
            "\n",
            "Genre: government\n",
            "freed: 0\n",
            "freedom: 20\n",
            "freedoms: 0\n",
            "freeholders: 0\n",
            "freely: 1\n",
            "freeman: 0\n",
            "freeman's: 0\n",
            "\n",
            "Genre: learned\n",
            "freed: 2\n",
            "freedom: 10\n",
            "freedoms: 0\n",
            "freeholders: 0\n",
            "freely: 2\n",
            "freeman: 1\n",
            "freeman's: 0\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "\n",
        "# Download brown corpus if not already\n",
        "nltk.download('brown')\n",
        "\n",
        "# a) Print all genres in the Brown corpus\n",
        "genres = brown.categories()\n",
        "print(\"All genres in Brown corpus:\")\n",
        "print(genres)\n",
        "\n",
        "# b) Select 'editorial' genre and get sorted unique word types\n",
        "editorial_words = set(w.lower() for w in brown.words(categories='editorial'))\n",
        "sorted_editorial_words = sorted(editorial_words)\n",
        "print(\"\\nNumber of unique words in 'editorial':\", len(sorted_editorial_words))\n",
        "\n",
        "# Print first 20 words as sample (optional)\n",
        "print(\"Sample words in 'editorial':\", sorted_editorial_words[:20])\n",
        "\n",
        "# c) Select range [3493:3499] from sorted list of editorial words\n",
        "selected_words = sorted_editorial_words[3493:3500]  # end index is exclusive, so +1\n",
        "print(\"\\nSelected words [3493:3499]:\", selected_words)\n",
        "\n",
        "# Genres to check counts in\n",
        "target_genres = ['fiction', 'lore', 'belles_lettres', 'government', 'learned']\n",
        "\n",
        "print(\"\\nWord counts in selected genres:\")\n",
        "for genre in target_genres:\n",
        "    # Get frequency distribution of words in this genre (lowercase)\n",
        "    fdist = nltk.FreqDist(w.lower() for w in brown.words(categories=genre))\n",
        "\n",
        "    # Count the occurrences of each selected word\n",
        "    counts = {word: fdist[word] for word in selected_words}\n",
        "    print(f\"\\nGenre: {genre}\")\n",
        "    for word, count in counts.items():\n",
        "        print(f\"{word}: {count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668
        },
        "id": "e1d08e37",
        "outputId": "fc25332f-1f11-4ff0-836b-220dc449a652"
      },
      "source": [
        "!pip install gensim"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n",
            "Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.2/38.2 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.16.2\n",
            "    Uninstalling scipy-1.16.2:\n",
            "      Successfully uninstalled scipy-1.16.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "tsfresh 0.21.1 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "scipy"
                ]
              },
              "id": "1ef8a0123f3342ed9fbbd0c6dac0cc99"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3da78022",
        "outputId": "794309c6-80f2-4fe7-a17d-881c1e83e908"
      },
      "source": [
        "import nltk\n",
        "nltk.download('toy_grammars')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Error loading toy_grammars: Package 'toy_grammars' not\n",
            "[nltk_data]     found in index\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ]
}